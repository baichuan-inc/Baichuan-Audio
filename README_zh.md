<div align="center">

<img src="./assets/logo.png" width="300em" ></img> 

## **å¼€æºç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’åŸºåº§**

  <strong>ä¸­æ–‡ |
  [English](./README.md)</strong>
  
  <p align="center">
  Baichuan-Audio <a href="https://huggingface.co/baichuan-inc/Baichuan-Audio-Instruct">ğŸ¤—</a> | Baichuan-Audio-Base <a href="https://huggingface.co/baichuan-inc/Baichuan-Audio-Base">ğŸ¤—</a>  | æŠ€æœ¯æŠ¥å‘Š <a href="https://arxiv.org/abs/2502.17239">ğŸ“–</a> 
</p>
</p>
<p align="center">
OpenAudioBench <a href="https://huggingface.co/datasets/baichuan-inc/openAudioBench">ğŸ¤—</a>  | è®­ç»ƒæ•°æ® <a href="#">ğŸ¤—</a> <small>(Coming Soon)</small>
</p>

  <!-- <p align="center">
    OpenMM-Medical <a href="https://huggingface.co/datasets/baichuan-inc/OpenMM_Medical">ğŸ¤—</a> | OpenAudioBench <a href="https://huggingface.co/datasets/baichuan-inc/OpenAudioBench">ğŸ¤—</a> 
</p> -->
</div>

## Baichuan-Audio

**Baichuan-Audio** æ˜¯Baichuanæœ€æ–°çš„ç«¯åˆ°ç«¯è®­ç»ƒçš„è¯­éŸ³äº¤äº’å¤§æ¨¡å‹ï¼Œæ— ç¼é›†æˆäº†éŸ³é¢‘ç†è§£å’Œç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒé«˜è´¨é‡å¯æ§çš„ä¸­è‹±åŒè¯­å®æ—¶å¯¹è¯ã€‚

- **Baichuan-Audio-Base**: ä¸ºä¿ƒè¿›è¯­éŸ³å¤§æ¨¡å‹å‘å±•ï¼Œæˆ‘ä»¬å¼€æºäº†ä½¿ç”¨é«˜è´¨é‡æµ·é‡æ•°æ®è®­ç»ƒçš„ç«¯åˆ°ç«¯è¯­éŸ³åŸºåº§æ¨¡å‹ã€‚è¯¥æ¨¡å‹æœªç»SFTæŒ‡ä»¤å¾®è°ƒï¼Œå¯å¡‘æ€§å¼ºã€‚

- **Baichuan-Audio**: æ¥å—æ–‡æœ¬ã€éŸ³é¢‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºï¼Œèƒ½å¤Ÿ**åœ¨ä¿æŒé¢„è®­ç»ƒLLMæ™ºå•†èƒ½åŠ›ä¸‹å®ç°æ— ç¼çš„é«˜è´¨é‡è¯­éŸ³äº¤äº’ï¼Œå’Œç”¨æˆ·è¿›è¡Œå®æ—¶è¯­éŸ³å¯¹è¯**ã€‚

- åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†éŸ³é¢‘ç†è§£å’Œç”ŸæˆåŸºå‡†ï¼ˆOpenAudio-Benchï¼‰ï¼Œä»¥è¯„ä¼°éŸ³é¢‘çš„ç«¯åˆ°ç«¯èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ•°æ®ä¹Ÿå³å°†å¼€æºã€‚


<br>

### Model Architecture

<div align="center">
<img src="./assets/audiollm.png" , width=85%>
</div>
<br>

**Baichuan-Audio** ä¸»è¦ç”± Baichuan-Audio Tokenizerã€Audio LLM å’ŒFlow-matching based Audio Decoder ä¸‰éƒ¨åˆ†ç»„æˆã€‚é¦–å…ˆè¯­éŸ³é€šè¿‡Baichuan-Audio Tokenizerè½¬æ¢ä¸ºç¦»æ•£éŸ³é¢‘ tokenã€‚ç„¶åï¼ŒAudio LLM ä»¥äº¤é”™æ–¹å¼ç”Ÿæˆå¯¹é½çš„æ–‡æœ¬å’ŒéŸ³é¢‘ tokenï¼Œå¹¶é€šè¿‡ç‰¹æ®Š token å®ç°æ–‡æœ¬å’ŒéŸ³é¢‘ä¹‹é—´çš„æ— ç¼æ¨¡æ€åˆ‡æ¢ã€‚éŸ³é¢‘ token ç”±ç‹¬ç«‹çš„ audio head å¤„ç†ï¼Œå¹¶ç”¨åŸºäºæµåŒ¹é…çš„éŸ³é¢‘è§£ç å™¨é‡å»ºé«˜è´¨é‡çš„æ¢…å°”é¢‘è°±å›¾ï¼Œæœ€åé€šè¿‡å£°ç å™¨å°†å…¶è½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢ã€‚

- Baichuan-Audio-Tokenizer é‡‡ç”¨ 12.5hz å¸§ç‡è®¾è®¡ã€‚å…¶ä½¿ç”¨ Whisper Large Encoder ä» Mel è°±ä¸­æå–é«˜çº§éŸ³é¢‘ç‰¹å¾ï¼Œç„¶åä½¿ç”¨ 8 å±‚ RVQ æ¥æœ€å¤§é™åº¦åœ°å‡å°‘é‡åŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚ä¸ºäº†åŒæ—¶æ•è·æ•è·è¯­ä¹‰å’Œå£°å­¦ä¿¡æ¯ï¼Œæˆ‘ä»¬åˆ†åˆ«é€šè¿‡ Mel è°±é‡æ„å’Œ Pre-trained LLM è¿›è¡Œå£°å­¦å’Œè¯­ä¹‰ç›‘ç£ã€‚
<div align="center">
<img src="./assets/vq.png" , width=30%>
</div>

- Audio LLM ä»¥äº¤é”™æ–¹å¼ç”Ÿæˆå¯¹é½çš„æ–‡æœ¬å’ŒéŸ³é¢‘ tokenï¼Œå¹¶é€šè¿‡ç‰¹æ®Š token å®ç°æ–‡æœ¬æ¨¡æ€å’ŒéŸ³é¢‘æ¨¡æ€ä¹‹é—´çš„æ— ç¼åˆ‡æ¢ã€‚éŸ³é¢‘ token ç”±ç‹¬ç«‹çš„ audio head å¤„ç†ã€‚

- Flow-matching based Audio Decoderç”¨æ¥é‡å»ºé«˜è´¨é‡çš„æ¢…å°”é¢‘è°±å›¾ã€‚è¯¥æ¨¡å‹åœ¨ 24 kHz éŸ³é¢‘ä¸Šè¿›è¡Œè®­ç»ƒä»¥ç”Ÿæˆç›®æ ‡æ¢…å°”å£°è°±å›¾ã€‚æœ€åé€šè¿‡å£°ç å™¨å°†å…¶è½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢ã€‚

<div align="center">
<img src="./assets/decoder.png" , width=24%>
</div>


### Pre-training details
- #### Pre-training data
éŸ³é¢‘è®­ç»ƒæ•°æ®å¤§è‡´å¯åˆ†ä¸ºä¸¤ç§ä¸»è¦ç±»å‹ï¼šéŸ³é¢‘ç†è§£æ•°æ®å’ŒéŸ³é¢‘ç”Ÿæˆæ•°æ®ã€‚
<div align="center">
<img src="./assets/table.png" , width=80%>
</div>

éŸ³é¢‘æ–‡æœ¬é…å¯¹æ•°æ®ï¼ˆä¾‹å¦‚ ASR å’Œ TTS æ•°æ®ï¼‰å¯æé«˜åŸºæœ¬è¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ã€‚å¦ä¸€æ–¹é¢ï¼Œçº¯éŸ³é¢‘æ•°æ®å¢å¼ºäº†ç‹¬ç«‹å¤„ç†éŸ³é¢‘æ¨¡æ€çš„èƒ½åŠ›ã€‚Audio-Text Interleaved æ•°æ®ç”±äº¤æ›¿çš„æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€ç»„æˆï¼Œç”±æ ‡ç‚¹ç¬¦å·åˆ†å‰²ä»¥ä¿ƒè¿›è·¨æ¨¡æ€çŸ¥è¯†ä¼ é€’ã€‚Interleaved Text-to-Speech æ•°æ®ç”±å®Œå…¨å¯¹é½çš„æ–‡æœ¬å’ŒéŸ³é¢‘å†…å®¹ç»„æˆï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹åœ¨æ–‡æœ¬ç›‘ç£ä¸‹ç”ŸæˆéŸ³é¢‘ token çš„èƒ½åŠ›ã€‚

äº¤é”™æ•°æ®é‡‡é›†æµç¨‹åˆ†ä¸ºçˆ¬å–å’Œåˆæˆä¸¤ç§ç±»å‹ï¼Œå…±è®¡è·å¾—äº† 142k å°æ—¶çš„ ITTS æ•°æ®å’Œ 393k å°æ—¶çš„ INTLV æ•°æ®ã€‚
<div align="center">
<img src="./assets/data.png" , width=80%>
</div>

<br>

- #### Two stage training strategy
è¯­éŸ³æ¨¡æ€ä¸æ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å†²çªå¯èƒ½ä¼šå¹²æ‰°é¢„è®­ç»ƒLLMä¸­é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è¡¨å¾ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹æ™ºå•†æ€§èƒ½é€€åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ç¼“è§£æ¨¡æ€ä¹‹é—´çš„è®­ç»ƒå†²çªã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒLLM çš„å‚æ•°ä¿æŒä¸å˜ï¼Œåªæ›´æ–° audio embedding layer å’Œ audio head çš„å‚æ•°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé™¤LM embedding layer å’Œ LM head çš„å‚æ•°å¤–ï¼Œæ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®­ç»ƒã€‚


### Local WebUI Demo

#### Preparation

##### Create a Virtual Environment
```bash
conda create -n baichuan_omni python==3.12
conda activate baichuan_omni
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
pip install accelerate flash_attn==2.6.3 speechbrain==1.0.0 deepspeed==0.14.4
apt install llvm ffmpeg
```
##### Download the Model and Modify the Model Path
ä¿®æ”¹ web_demo/constants.py ä¸­çš„ MODEL_PATH ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„

#### ASR and TTS Demo

```bash
cd web_demo
python base_asr_demo.py
python base_tts_demo.py
```
#### Speech interaction Demo

```bash
cd web_demo
python s2s_gradio_demo_cosy_multiturn.py
```

### Cases


ä»¥ä¸‹æ˜¯ä¸€ä¸ªéŸ³é¢‘è¾“å…¥å’ŒéŸ³é¢‘è¾“å‡ºçš„ç¤ºä¾‹:

| **è¾“å…¥ç±»å‹** | **è¾“å…¥å†…å®¹**   | **è¾“å‡ºç±»å‹** | **è¾“å‡ºå†…å®¹**                                                                                   |
|----------------|---------------------|-----------------|------------------------------------------------------------------------------------------------------|
| éŸ³é¢‘          | "ä»‹ç»ä¸‹åŒ—äº¬" | éŸ³é¢‘           | [éŸ³é¢‘è¾“å‡º](https://raw.githubusercontent.com/baichuan-inc/Baichuan-Audio/refs/heads/main/assets/audio_out.wav) |


### Open-Source Evaluation Set

**OpenAudioBench**

ä¸ºäº†æ›´é«˜æ•ˆçš„è¯„ä¼°æ¨¡å‹çš„â€œæ™ºå•†â€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº† OpenAudioBenchï¼Œå…±åŒ…å«5ä¸ªéŸ³é¢‘ç«¯åˆ°ç«¯ç†è§£å­è¯„æµ‹é›†ï¼Œåˆ†åˆ«æ˜¯4ä¸ªå…¬å¼€è¯„æµ‹é›†ï¼ˆllama questionã€WEB QAã€TriviaQAã€AlpacaEvalï¼‰ï¼Œä»¥åŠç™¾å·å›¢é˜Ÿè‡ªå»ºçš„è¯­éŸ³é€»è¾‘æ¨ç†è¯„æµ‹é›†ï¼Œå…±2701æ¡æ•°æ®ï¼Œèƒ½å¤Ÿç»¼åˆåæ˜ æ¨¡å‹â€œæ™ºå•†â€æ°´å¹³ã€‚

### Model performance

<div align="center">
<img src="./assets/result.png" , width=90%>
</div>


### Acknowledgments

- è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASR, Automatic Speech Recognitionï¼‰æ¨¡å‹ï¼šã€Whisperã€‘(https://github.com/openai/whisper)
- å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼šã€Qwen2.5 7Bã€‘(https://arxiv.org/abs/2412.15115)
- éƒ¨åˆ†ä»£ç æ¥è‡ªï¼šCosyVoiceå’ŒMatcha-TTSï¼š(https://github.com/FunAudioLLM/CosyVoice, https://github.com/shivammehta25/Matcha-TTS/)
- ä½¿ç”¨CosyVoice 2.0ä¸­çš„HiFi-GAN vocoderï¼š(https://funaudiollm.github.io/cosyvoice2/)


### License
Baichuan-Audio-Base/Baichuan-Audio æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Apache 2.0](https://github.com/baichuan-inc/Baichuan-Audio/blob/main/LICENSE)


### Citation

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬æ¨¡å‹/ä»£ç /è®ºæ–‡æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ â­ å’Œ å¼•ç”¨ ğŸ“ï¼Œæ„Ÿè°¢ï¼

```bib
@article{li2025baichuan,
  title={Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction},
  author={Li, Tianpeng and Liu, Jun and Zhang, Tao and Fang, Yuanbo and Pan, Da and Wang, Mingrui and Liang, Zheng and Li, Zehuan and Lin, Mingan and Dong, Guosheng and others},
  journal={arXiv preprint arXiv:2502.17239},
  year={2025}
}
```